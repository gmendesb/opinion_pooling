# Choosing the weights for the logarithmic pooling of probability distributions
## [World Statistics Congress, Rio de Janeiro, Brazil, July 26-30, 2015](http://www.isi2015.org/)
[Luiz Max de Carvalho](http://lmfcarvalho.org/about/)<sup>1</sup>, [Daniel Villela](http://www.procc.fiocruz.br/Members/dvillela), [Flavio Coelho](http://fccoelho.github.io/)<sup>2</sup> and [Leonardo Bastos](http://www.procc.fiocruz.br/Members/lsbastos) <sup>1</sup>
 
<sup>1</sup> Program for Scientific Computing, Oswaldo Cruz Foundation, Rio de Janeiro -- RJ, Brazil.
<sup>2</sup> School of Applied Mathematics, Getulio Vargas Foundation (FGV), Rio de Janeiro -- RJ, Brazil.



## Abstract ([Arxviv](http://arxiv.org/abs/1502.04206))

Combining different prior distributions is an important issue in decision theory and Bayesian inference.
Logarithmic pooling is a popular method to aggregate expert opinions by using a set of weights that reflect the reliability of each information source.
The resulting pooled distribution however heavily depends set of weights given to each opinion/prior.
In this paper we explore three objective approaches to assigning weights to opinions. Two methods are stated in terms of optimization problems and a third one uses a hierarchical prior that accounts for uncertainty on the weights. 
We explore an example in which a proportion is estimated using weights assigned to a finite set of opinions and show that, depending on the used method, results vary from discarding some of the expert opinions to the situation in which all opinions are assigned equal weights.
Nevertheless, the three methods explored in this paper lead to very similar combined priors, with very similar integrated (marginal) likelihoods.

Key-words: logarithmic pooling; expert opinion; maximum entropy; Kullback-Liebler divergence; Dirichlet prior. 

![](figures/new_beta_example.png)
